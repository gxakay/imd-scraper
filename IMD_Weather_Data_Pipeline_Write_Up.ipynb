{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IMD Weather Data Pipeline : Web Scraping With Python [ Write Up ]"
      ],
      "metadata": {
        "id": "xGfw42Vdv-t-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Indian Meteorological Department (IMD) website provides vital weather information for India. Extracting this data manually is impractical. This article will demonstrate how to build a Python web scraper to automatically collect weather data from the IMD website. We'll cover the necessary libraries and techniques to get you started and tap into a wealth of meteorological information."
      ],
      "metadata": {
        "id": "J8CX1M25xutG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without further ado, let's get started!\n",
        "\n",
        "1. Setting up the Tools (Import Libraries):\n",
        "First, we're importing the necessary libraries. requests to fetch web pages, BeautifulSoup to parse HTML, pandas to handle and organize our data into tables, and os to interact with the file system."
      ],
      "metadata": {
        "id": "cwSPhkB9yxqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    import pandas as pd\n",
        "    import os"
      ],
      "metadata": {
        "id": "3JqpM8DPzOh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Defining our Target (Main URL):\n",
        "We're setting the address of our target webpage to the variable main_url. This is where we'll start our web scraping adventure. It's the same menu_test.php page on the India Meteorological Department website."
      ],
      "metadata": {
        "id": "CXVQhw19zVOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_url = 'https://city.imd.gov.in/citywx/menu_test.php'"
      ],
      "metadata": {
        "id": "n2cUuczIzlVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Naming our Data Storage (Excel File):\n",
        "Next, we're creating a variable called excel_file, that is set to scraped_tables.xlsx. This is the excel file where we'll be storing our collected weather data."
      ],
      "metadata": {
        "id": "agxuqSbDzo0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excel_file = \"scraped_tables.xlsx\""
      ],
      "metadata": {
        "id": "s2WqWqOjzuYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Preparing the Container (Empty all_tables List):\n",
        "We create an empty list called all_tables. This list will hold the pandas dataframes that we will be collecting. We'll be adding all of them together at the end into one big dataframe."
      ],
      "metadata": {
        "id": "JdgfXWDZzyce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tables = []"
      ],
      "metadata": {
        "id": "0MEKDUW9z3Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Checking for Existing Data:\n",
        "Now, we're doing something clever. We're checking if an Excel file called scraped_tables.xlsx already exists. We do this using the os.path.exists() function. If the file exists, it means we might already have some scraped data we want to keep."
      ],
      "metadata": {
        "id": "BWGbBgcM0AC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(excel_file):"
      ],
      "metadata": {
        "id": "p6eFeDNV0F7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Loading Existing Data (if available):\n",
        "If the Excel file exists, we try to load its content into a pandas DataFrame, called existing_df. We wrap this in a try-except block to handle potential errors, like if the file is corrupted or not a valid Excel file. We then load this dataframe into the all_tables list so that the new data can be appended to the already existing data. Finally, we print a message that informs the user we are resuming scraping, and if any error occurs then it lets the user know we are starting from scratch.We use a try block to catch potential errors during the web fetching and parsing process."
      ],
      "metadata": {
        "id": "ezQQNR280KJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    try:\n",
        "            existing_df = pd.read_excel(excel_file)\n",
        "            all_tables = [existing_df]\n",
        "            print(\"Resuming scraping from existing data in scraped_tables.xlsx\")\n",
        "    except Exception as e:\n",
        "            print(f\"Error reading existing data: {e}. Starting from scratch.\")"
      ],
      "metadata": {
        "id": "W0Aefu2F0U7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Fetching Main Webpage Content:\n",
        "Inside the try block, we use requests.get to fetch the content of the main webpage, and save it into html_content variable."
      ],
      "metadata": {
        "id": "jV23rAWT09ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(main_url)\n",
        "html_content = response.text"
      ],
      "metadata": {
        "id": "khAuyrYd1Pvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Parsing HTML with BeautifulSoup:\n",
        "We then create the BeautifulSoup object to make sense of the HTML structure of the webpage."
      ],
      "metadata": {
        "id": "zh8YY9Zb1Uvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(html_content, 'html.parser')"
      ],
      "metadata": {
        "id": "REud1tMu1c_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Finding Relevant Links (using a lambda function):\n",
        "Next, we need to get the links of the specific pages that contain the tables we need. We do this by extracting all the < a > elements with href attributes starting with the string \"city_weather_test_try_warnings.php?id=\".\n",
        "Now we use a lambda function to check that condition,and save this to the links list"
      ],
      "metadata": {
        "id": "YtnFtJki18WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links = [a['href'] for a in soup.find_all('a', href=lambda href: href and href.startswith('city_weather_test_try_warnings.php?id='))]"
      ],
      "metadata": {
        "id": "FismQNxn1toZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Looping Through Each Link:\n",
        "We start iterating over every link in the links list."
      ],
      "metadata": {
        "id": "qARufILg1h3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for link in links:"
      ],
      "metadata": {
        "id": "skS24wYr2Jwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Constructing Full URL:\n",
        "For each link, we create the complete URL by prepending the base url using string concatenation, which is stored in full_url."
      ],
      "metadata": {
        "id": "GdkgXglP2NO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_url = 'https://city.imd.gov.in/citywx/' + link"
      ],
      "metadata": {
        "id": "g-hWMejg2RvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Checking if Data Already Exists (in Excel):\n",
        "We then check if the table has already been collected before, and if the existing df is not empty. This prevents the program from loading the same tables if we already scraped them. If the full_url is already in the column names of the existing dataframe, we skip the link and move on. This assumes that we are storing the full url of the table in a column. The idea behind this code is to resume from where it stops, this saves us a bit of time."
      ],
      "metadata": {
        "id": "fBMI7EFU2Wng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if existing_df is not None and full_url in existing_df.columns:\n",
        "   print(f\"Skipping URL: {full_url} - Data already exists.\")\n",
        "   continue"
      ],
      "metadata": {
        "id": "dEMqeu0Q22NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Attempting to Read HTML Tables (Try Block):\n",
        "We use another try block to load the html table into the variable called tables, this can fail so we want to catch those failures."
      ],
      "metadata": {
        "id": "MG6PcgqO3NBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    tables = pd.read_html(full_url)"
      ],
      "metadata": {
        "id": "v3eBG5533Ybl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Skipping Pages with Insufficient Tables:\n",
        "We check if the tables contains at least two tables, and if not, we print a message saying that the table is being skipped."
      ],
      "metadata": {
        "id": "x_f1l75i3ed_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "           if len(tables) < 2:\n",
        "                print(f\"Skipping URL: {full_url} - Insufficient tables.\")\n",
        "                continue"
      ],
      "metadata": {
        "id": "RY7dIV_F6k1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Extracting Relevant Tables:\n",
        "We load all tables that are not the first one into the all_tables list. We only skip the first table because we are only interested in the tables with the data, not the first one with the location."
      ],
      "metadata": {
        "id": "urbVFdAW3m1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for table in tables[1:]:\n",
        "    all_tables.append(table)"
      ],
      "metadata": {
        "id": "ZeQ8y1H-3qmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Handling Errors Loading Tables:\n",
        "If any ValueError is raised, we catch it here and print a message to the console letting us know which table could not be loaded, and we skip to the next link using the continue keyword."
      ],
      "metadata": {
        "id": "RO8Endjo3204"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "        except ValueError as e:\n",
        "            print(f\"Error reading tables from {full_url}: {e}\")\n",
        "            continue"
      ],
      "metadata": {
        "id": "Gn3cuLOF36j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Handling Errors Fetching/Parsing Main Page:\n",
        "We catch any kind of exception outside the inner loop, so we print an error if we can't fetch or parse the html of the main page."
      ],
      "metadata": {
        "id": "GT4XSftQ3_5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "pb5-LZLo4EsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Saving Data to Excel (Finally Block):\n",
        "Finally, we have a finally block, which ensures that this part always executes, regardless of whether any exceptions occurred in the try block. If all_tables is not empty, we combine them all together into a new dataframe called final_df, and save it to the excel file. We also print a message letting the user know the excel file has been updated. Or, if no tables were scraped, or an error occurred, we print a message to the console that no tables were found during scraping."
      ],
      "metadata": {
        "id": "2-C8BRcb4IKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    finally:\n",
        "\n",
        "        if all_tables:\n",
        "            final_df = pd.concat(all_tables, ignore_index=True)\n",
        "            final_df.to_excel(excel_file, index=False)\n",
        "            print(\"Data saved to scraped_tables.xlsx\")\n",
        "        else:\n",
        "            print(\"No tables found or an error occurred during scraping.\")"
      ],
      "metadata": {
        "id": "hAJID6rC4NVm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}